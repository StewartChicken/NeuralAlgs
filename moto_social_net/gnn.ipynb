{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done: \n",
    "# Graph and data generation\n",
    "# Network Architecture\n",
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9efdef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ed15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96073b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, in_dim)\n",
    "        return self.net(x)  # (N, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b0fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # Message function φ: R^dim -> R^dim\n",
    "        self.msg_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        # Update function ψ: R^(2*dim) -> R^dim\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, H, edge_index):\n",
    "        \"\"\"\n",
    "        H: (N, dim) node embeddings at current layer\n",
    "        edge_index: (2, E) tensor with [src; dst]\n",
    "        \"\"\"\n",
    "        src, dst = edge_index  # each: (E,)\n",
    "\n",
    "        # 1. Messages from src nodes along edges\n",
    "        h_src = H[src]              # (E, dim)\n",
    "        m = self.msg_mlp(h_src)     # (E, dim)\n",
    "\n",
    "        # 2. Aggregate messages at dst nodes by summation\n",
    "        N, dim = H.shape\n",
    "        agg = torch.zeros_like(H)   # (N, dim)\n",
    "        agg.index_add_(0, dst, m)   # sum messages into dst indices\n",
    "\n",
    "        # 3. Update node states using previous state + aggregated message\n",
    "        h_cat = torch.cat([H, agg], dim=-1)  # (N, 2*dim)\n",
    "        H_next = self.update_mlp(h_cat)      # (N, dim)\n",
    "\n",
    "        return H_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5e7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)   # output logit per node\n",
    "        )\n",
    "\n",
    "    def forward(self, H):\n",
    "        # H: (N, in_dim)\n",
    "        logits = self.net(H).squeeze(-1)  # (N,)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd883d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our MPNN model for classifying influencers and non-influencers\n",
    "class SocialGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, emb_dim=32, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_dim, hidden_dim, emb_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MPNNLayer(emb_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.decoder = Decoder(emb_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: (N, in_dim) node features\n",
    "        edge_index: (2, E) edge list\n",
    "        \"\"\"\n",
    "        # 1. Encode features to latent space\n",
    "        H = self.encoder(x)  # (N, emb_dim)\n",
    "\n",
    "        # 2. Apply K message passing layers\n",
    "        for layer in self.layers:\n",
    "            H = layer(H, edge_index)\n",
    "\n",
    "        # 3. Decode to logits\n",
    "        logits = self.decoder(H)  # (N,)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67813919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "feature_names = [\n",
    "    'normalized_degree',\n",
    "    'clustering_coefficient',\n",
    "    'posts_per_day',\n",
    "    'likes_per_post',\n",
    "    'follower_ratio',\n",
    "    'age',\n",
    "    'years_riding',\n",
    "    'miles_ridden',\n",
    "    'bikes_owned',\n",
    "    'avg_displacement',\n",
    "    'avg_msrp',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555e4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_dir(graph_dir):\n",
    "    \"\"\"\n",
    "    graph_dir: path like './data/train/000'\n",
    "    Returns: X (N,F), edge_index (2,E), y (N,)\n",
    "    \"\"\"\n",
    "    nodes_path = os.path.join(graph_dir, \"nodes.csv\")\n",
    "    edges_path = os.path.join(graph_dir, \"edges.csv\")\n",
    "\n",
    "    nodes_df = pd.read_csv(nodes_path)\n",
    "    edges_df = pd.read_csv(edges_path)\n",
    "\n",
    "    # Node features and labels\n",
    "    X_np = nodes_df[feature_names].values          # (N, F)\n",
    "    y_np = nodes_df[\"label\"].values               # (N,)\n",
    "\n",
    "    X = torch.tensor(X_np, dtype=torch.float32)   # (N, F)\n",
    "    y = torch.tensor(y_np, dtype=torch.float32)   # (N,)\n",
    "\n",
    "    # Edge list\n",
    "    src = torch.tensor(edges_df[\"src\"].values, dtype=torch.long)\n",
    "    dst = torch.tensor(edges_df[\"dst\"].values, dtype=torch.long)\n",
    "\n",
    "    # For now: one direction per edge; you can symmetrize later if you want\n",
    "    edge_index = torch.stack([src, dst], dim=0)   # (2, E)\n",
    "\n",
    "    return X, edge_index, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e7d675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 training graphs\n",
      "Example: ['./data/train\\\\000', './data/train\\\\001', './data/train\\\\002']\n",
      "Found 10 validation graphs\n",
      "Example: ['./data/validate\\\\000', './data/validate\\\\001', './data/validate\\\\002']\n",
      "Found 100 test graphs\n",
      "Example: ['./data/test\\\\000', './data/test\\\\001', './data/test\\\\002']\n"
     ]
    }
   ],
   "source": [
    "train_root = \"./data/train\"\n",
    "val_root = \"./data/validate\"\n",
    "test_root = \"./data/test\"\n",
    "\n",
    "# All subdirectories inside ./data/train\n",
    "train_graph_dirs = sorted(\n",
    "    d for d in glob.glob(os.path.join(train_root, \"*\"))\n",
    "    if os.path.isdir(d)\n",
    ")\n",
    "\n",
    "val_graph_dirs = sorted(\n",
    "    d for d in glob.glob(os.path.join(val_root, \"*\"))\n",
    "    if os.path.isdir(d)\n",
    ")\n",
    "\n",
    "test_graph_dirs = sorted(\n",
    "    d for d in glob.glob(os.path.join(test_root, \"*\"))\n",
    "    if os.path.isdir(d)\n",
    ")\n",
    "\n",
    "print(\"Found\", len(train_graph_dirs), \"training graphs\")\n",
    "print(\"Example:\", train_graph_dirs[:3])\n",
    "print(\"Found\", len(val_graph_dirs), \"validation graphs\")\n",
    "print(\"Example:\", val_graph_dirs[:3])\n",
    "print(\"Found\", len(test_graph_dirs), \"test graphs\")\n",
    "print(\"Example:\", test_graph_dirs[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e87b8113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: torch.Size([20])\n",
      "Unique labels: (tensor([0., 1.]), tensor([17,  3]))\n",
      "Positive rate: 0.15000000596046448\n"
     ]
    }
   ],
   "source": [
    "# This cell checks the ratio between influencers and non-influencers.\n",
    "\n",
    "# Check a single graph\n",
    "if True:\n",
    "    check_graph = 4\n",
    "\n",
    "    X, edge_index, y = load_graph_from_dir(train_graph_dirs[check_graph])\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"Unique labels:\", torch.unique(y, return_counts=True))\n",
    "    print(\"Positive rate:\", y.float().mean().item())\n",
    "\n",
    "# Check all graphs in a directory\n",
    "if False:\n",
    "    for i in range(10): # Change to match num graphs in selected directory\n",
    "        X, edge_index, y = load_graph_from_dir(val_graph_dirs[i]) # Change the directory (train, val, test)\n",
    "        print(\"y shape:\", y.shape)\n",
    "        print(\"Unique labels:\", torch.unique(y, return_counts=True))\n",
    "        print(\"Positive rate:\", y.float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e008066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one graph to deduce input dimension\n",
    "X_example, edge_index_example, y_example = load_graph_from_dir(train_graph_dirs[0])\n",
    "in_dim = X_example.shape[1]      # should be 11\n",
    "\n",
    "model = SocialGNN(in_dim=in_dim, hidden_dim=64, emb_dim=32, num_layers=3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce3f0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Check the input dimension. This should be 11 since each node has 11 features\n",
    "print(in_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b6eb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the model is on the GPU\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc53ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train loss/node: 0.5283 | Val loss/node: 0.4250 | Val acc: 0.795\n",
      "Epoch 002 | Train loss/node: 0.4075 | Val loss/node: 0.2839 | Val acc: 0.870\n",
      "Epoch 003 | Train loss/node: 0.2252 | Val loss/node: 0.3348 | Val acc: 0.860\n",
      "Epoch 004 | Train loss/node: 0.2145 | Val loss/node: 0.1918 | Val acc: 0.925\n",
      "Epoch 005 | Train loss/node: 0.1551 | Val loss/node: 0.1455 | Val acc: 0.940\n",
      "Epoch 006 | Train loss/node: 0.1125 | Val loss/node: 0.1058 | Val acc: 0.955\n",
      "Epoch 007 | Train loss/node: 0.0991 | Val loss/node: 0.0982 | Val acc: 0.950\n",
      "Epoch 008 | Train loss/node: 0.1323 | Val loss/node: 0.1707 | Val acc: 0.915\n",
      "Epoch 009 | Train loss/node: 0.1679 | Val loss/node: 0.1193 | Val acc: 0.960\n",
      "Epoch 010 | Train loss/node: 0.0993 | Val loss/node: 0.0783 | Val acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------- TRAINING ----------\n",
    "    model.train()\n",
    "\n",
    "    train_total_loss  = 0.0\n",
    "    train_total_nodes  = 0\n",
    "\n",
    "    for graph_dir in train_graph_dirs:\n",
    "        X, edge_index, y = load_graph_from_dir(graph_dir)\n",
    "\n",
    "        # Move to GPU/CPU\n",
    "        X = X.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X, edge_index)    # (N,)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        #loss.backward(lr=1e-5) # This is the learning rate used in the paper 'Neural Execution of Graph Algorithms' \n",
    "        optimizer.step()\n",
    "\n",
    "        train_total_loss  += loss.item() * X.size(0)  # weight by #nodes\n",
    "        train_total_nodes  += X.size(0)\n",
    "\n",
    "    train_avg_loss = train_total_loss  / train_total_nodes \n",
    "    \n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_total_nodes = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph_dir in val_graph_dirs:\n",
    "            X, edge_index, y = load_graph_from_dir(graph_dir)\n",
    "\n",
    "            X = X.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X, edge_index)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_total_loss += loss.item() * X.size(0)\n",
    "            val_total_nodes += X.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    val_avg_loss = val_total_loss / val_total_nodes if val_total_nodes > 0 else 0.0\n",
    "    val_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"Train loss/node: {train_avg_loss:.4f} | \"\n",
    "        f\"Val loss/node: {val_avg_loss:.4f} | \"\n",
    "        f\"Val acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd75c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset weights to random\n",
    "def reset_model(model):\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "989932ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "073a9db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 100 graphs\n",
      "Test Accuracy: 0.9715\n"
     ]
    }
   ],
   "source": [
    "# ---------- TESTING ----------\n",
    "\n",
    "model.eval()          # evaluation mode\n",
    "total_correct = 0\n",
    "total_nodes = 0\n",
    "\n",
    "num_graphs_tested = 0\n",
    "\n",
    "with torch.no_grad():  # no gradient tracking\n",
    "    for graph_dir in test_graph_dirs:\n",
    "        num_graphs_tested += 1\n",
    "        X, edge_index, y = load_graph_from_dir(graph_dir)\n",
    "\n",
    "        X = X.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(X, edge_index)\n",
    "\n",
    "        # Convert logits → probabilities → predicted class\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).long()   # threshold at 0.5\n",
    "\n",
    "        # Count accuracy\n",
    "        total_correct += (preds == y.long()).sum().item()\n",
    "        total_nodes += y.numel()\n",
    "\n",
    "test_acc = total_correct / total_nodes\n",
    "print(f\"Tested {num_graphs_tested} graphs\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
